{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bb5e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b68a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "appName = \"Sentiment Analysis with CNN in Spark\"\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8cb4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with automatically inferred schema\n",
    "tweets_csv = spark.read.csv(r\"C:\\Users\\DELL\\Downloads\\sparkproject\\sparkproject\\project\\dataset\\tweets.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc7f1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only \"SentimentText\" and \"Sentiment\" column, and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af9c1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only \"SentimentText\" and \"Sentiment\" column, and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1188918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3])\n",
    "trainingData = dividedData[0]  \n",
    "testingData = dividedData[1]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d357a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras tokenizer\n",
    "keras_tokenizer = KerasTokenizer()\n",
    "# Collect the text data from the DataFrame column and pass it to the tokenizer\n",
    "texts = trainingData.select(\"SentimentText\").rdd.flatMap(lambda x: x).collect()\n",
    "keras_tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fefe6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary size\n",
    "vocab_size = len(keras_tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text data to sequences\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(texts)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(testingData.select(\"SentimentText\").rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d194030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure uniform length\n",
    "maxlen = max(len(seq) for seq in train_sequences)\n",
    "train_data = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b58460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "embedding_dim = 50\n",
    "filters = 250\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56ed31a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d_2               │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d_2               │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fad31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history=model.fit(train_data, np.array(trainingData.select(\"label\").collect()), epochs=5, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_data, np.array(testingData.select(\"label\").collect()))\n",
    "\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03523f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "predictions_proba = model.predict(test_data)\n",
    "predictions = (predictions_proba > 0.5).astype(int)\n",
    "\n",
    "# Get the true labels\n",
    "true_labels = np.array(testingData.select(\"label\").collect())\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f955d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define the input text data\n",
    "input_text = [\"This is a great movie!\",\n",
    "              \"I didn't like this film at all.\"]\n",
    "\n",
    "# Convert text data to sequences using the Keras tokenizer\n",
    "input_sequences = keras_tokenizer.texts_to_sequences(input_text)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "maxlen = max(len(seq) for seq in input_sequences)\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=maxlen)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions_proba = model.predict(padded_input_sequences)\n",
    "predictions = (predictions_proba > 0.5).astype(int)\n",
    "\n",
    "# Print the predictions\n",
    "for i, text in enumerate(input_text):\n",
    "    print(f\"Input Text: {text}\")\n",
    "    print(f\"Predicted Sentiment: {'Positive' if predictions[i] == 1 else 'Negative'} (Probability: {predictions_proba[i][0]:.4f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2d78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
